save_dir: workspace/nanodet-plus-m_320_album
model:
  weight_averager:
    name: ExpMovingAverager
    decay: 0.9998
  arch:
    name: NanoDetPlus
    detach_epoch: 10
    backbone:
      name: TIMMWrapper
      model_name: tf_mobilenetv3_large_minimal_100.in1k
      features_only: True
      pretrained: True
      output_stride: 32
      out_indices: [2, 3, 4]
    fpn:
      name: GhostPAN
      in_channels: [40, 112, 960]
      out_channels: 96
      kernel_size: 5
      num_extra_level: 1
      use_depthwise: True
      activation: LeakyReLU
    head:
      name: NanoDetPlusHead
      num_classes: 1
      input_channel: 96
      feat_channels: 96
      stacked_convs: 2
      kernel_size: 5
      strides: [8, 16, 32, 64]
      activation: LeakyReLU
      reg_max: 7
      norm_cfg:
        type: BN
      loss:
        loss_qfl:
          name: QualityFocalLoss
          use_sigmoid: True
          beta: 2.0
          loss_weight: 1.0
        loss_dfl:
          name: DistributionFocalLoss
          loss_weight: 0.25
        loss_bbox:
          name: GIoULoss
          loss_weight: 2.0
    aux_head:
      name: SimpleConvHead
      num_classes: 1
      input_channel: 192
      feat_channels: 192
      stacked_convs: 4
      strides: [8, 16, 32, 64]
      activation: LeakyReLU
      reg_max: 7
data:
  train:
    name: CocoDataset
    img_path: Data\Benchmark\Preprocessed\Preprocessed_People_Detection.v8i.coco\train
    ann_path: Data\Benchmark\Preprocessed\Preprocessed_People_Detection.v8i.coco\train\train_cleaned_annotations.coco.json
    input_size: [320,320] #[w,h]
    keep_ratio: False
    pipeline:
      is_train: true
      name: AlbumentationsTransform
      keep_ratio: False
      perspective_prob: 0.0
      rotate_prob: 0.5
      flip_prob: 0.5
      brightness_contrast_prob: 0.5
      hue_saturation_prob: 0.0
      normalize: [[103.53, 116.28, 123.675], [57.375, 57.12, 58.395]]
      affine:
        rotate: [-3, 3]
        translate_percent: [-0.1, 0.1]
        scale: 0.8
        shear: [-1.0, 1.0]
        border_mode: constant
        mask_value: 0
        p: 0.6
      perspective:
        scale: [0.0, 0.01]
        p: 0.05
        border_mode: constant
      coarse_dropout:
        num_holes_range: [1, 8]
        hole_height_range: [10, 25]
        hole_width_range: [10, 25]
        fill_value: 0
        p: 0.3
      blur:
        blur_limit: [3, 7]
        p: 0.01
      median_blur:
        blur_limit: [3, 7]
        p: 0.01
      to_gray:
        p: 0.01
      clahe:
        clip_limit: 2.0
        tile_grid_size: [8, 8]
        p: 0.01
      iso_noise:
        color_shift: [0.01, 0.05]
        intensity: [0.1, 0.5]
        p: 0.2
      motion_blur:
        blur_limit: [3, 7]
        p: 0.1
      random_gamma:
        gamma_limit: [80, 120]
        p: 0.2
  val:
    name: CocoDataset
    img_path: Data\Benchmark\Preprocessed\Preprocessed_People_Detection.v8i.coco\valid
    ann_path: Data\Benchmark\Preprocessed\Preprocessed_People_Detection.v8i.coco\valid\val_cleaned_annotations.coco.json
    input_size: [320,320] #[w,h]
    keep_ratio: False
    pipeline:
      normalize: [[103.53, 116.28, 123.675], [57.375, 57.12, 58.395]]
device:
  gpu_ids: [0] # Set like [0, 1, 2, 3] if you have multi-GPUs
  workers_per_gpu: 4
  batchsize_per_gpu: 32
  precision: 32 # set to 16 to use AMP training
schedule:
  optimizer:
    name: AdamW
    lr: 0.0001
    weight_decay: 0.05
  warmup:
    name: linear
    steps: 500
    ratio: 0.0001
  total_epochs: 100
  lr_schedule:
    name: CosineAnnealingLR
    T_max: 100
    eta_min: 0.000001
  val_intervals: 1
grad_clip: 35
evaluator:
  name: CocoDetectionEvaluator
  save_key: mAP
log:
    interval: 1 #Log training process after n step
    train_example_id: 1 #Show the data[i] in the the train set 

class_names: ['person']
